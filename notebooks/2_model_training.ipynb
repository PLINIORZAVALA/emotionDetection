{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71edc7b",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "El dataset usado se denomina \"Facial Emotion Recognition Dataset\" y se encuentra público en Kaggle(La data mostrada en este proyecto ya se encuentra pre procesada). Puede encontrarlo en el siguiente enlace: https://www.kaggle.com/datasets/tapakah68/facial-emotion-recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e45cb",
   "metadata": {},
   "source": [
    "Tipos de emociones en la clasificación:\n",
    "- Anger \n",
    "- Contempt\n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy\n",
    "- Neutral\n",
    "- Sad\n",
    "- Surprised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb9ef2",
   "metadata": {},
   "source": [
    "1. Las librerías para la instalcion son keras, y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8758de7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instalación de librerías \n",
    "!pip install --upgrade keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c882e",
   "metadata": {},
   "source": [
    "2. Configuración de dataset:\n",
    "Definimos algunos parámetros importantes como: tamaño de imagenes, número de clases, etiquetas de las clases, número de epocas, batch size. Finalmente configuramos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fa820",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# Definimos algunos parámetros importantes\n",
    "width_shape = 48\n",
    "height_shape = 48\n",
    "num_classes = 7\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "class_names = ['angry','disgust','fear','happy','neutral','sad','surprise']\n",
    "\n",
    "# Configuramos el dataset de entrenamiento y validación\n",
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(  \n",
    "    train_data_dir,\n",
    "    target_size=(width_shape, height_shape),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',shuffle=True)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(  \n",
    "    val_data_dir,\n",
    "    target_size=(width_shape, height_shape),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1fd7c",
   "metadata": {},
   "source": [
    "3. Modelo CNN:\n",
    "Definimos la arquitectura del modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb6e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, Input, AveragePooling2D,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## Extracción de Características\n",
    "model.add(Conv2D(32,(3,3),padding = 'same',input_shape = (width_shape,height_shape,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(5,5),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.2))\n",
    "\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "## Clasificación\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Revisamos el modelo CNN\n",
    "model.summary()\n",
    "\n",
    "# Compilamos y estamos listos para el entrenamiento\n",
    "opt = Adam(learning_rate=1e-4, decay=1e-4 / epochs)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa62f6f4",
   "metadata": {},
   "source": [
    "4. Configuración de tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e31659",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configuración Tensorboard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime, os\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2758e",
   "metadata": {},
   "source": [
    "5. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4daad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Entrenamiento de la red\n",
    "model.fit(  \n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.n//batch_size,\n",
    "    validation_steps=val_generator.n//batch_size,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0840e",
   "metadata": {},
   "source": [
    "6. Curvas de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab1475",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdc4b2",
   "metadata": {},
   "source": [
    "7. Grabamos nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e04901",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"modelFEC.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce88d7",
   "metadata": {},
   "source": [
    "8. Prueba del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adfe40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "faces = []\n",
    "\n",
    "# Cargamos una imagen del directorio\n",
    "imaget_path = \"/content/images/validation/surprise/10185.jpg\"\n",
    "\n",
    "# Redimensionamos la imagen y convertimos a gray\n",
    "face = cv2.cvtColor(cv2.imread(imaget_path), cv2.COLOR_BGR2GRAY)\n",
    "face = cv2.resize(face, (48, 48))\n",
    "face2 = img_to_array(face)\n",
    "face2 = np.expand_dims(face2,axis=0)\n",
    "\n",
    "faces.append(face2)\n",
    "\n",
    "# El modelo estima la predicción\n",
    "preds = model.predict(faces)\n",
    "\n",
    "print(class_names[np.argmax(preds)])\n",
    "plt.imshow(cv2.cvtColor(np.asarray(face),cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb5f6c",
   "metadata": {},
   "source": [
    "9. Matriz de confución y métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ff8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Configuración del dataset de validación sin shuffle\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_generator = val_datagen.flow_from_directory(  \n",
    "    val_data_dir,\n",
    "    target_size=(width_shape, height_shape),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',shuffle=False)\n",
    "\n",
    "predictions = model.predict(val_generator)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_real = val_generator.classes\n",
    "\n",
    "matc=confusion_matrix(y_real, y_pred)\n",
    "\n",
    "plot_confusion_matrix(conf_mat=matc, figsize=(5,5), show_normed=False)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(metrics.classification_report(y_real,y_pred, digits = 4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
